{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from 64 channels loaded...!\n",
      "Objects created, now loading Transformer layer features...!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akamsali/anaconda3/envs/research_env/lib/python3.8/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /opt/conda/conda-bld/pytorch_1631630815121/work/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from auditory_cortex.Regression import transformer_regression\n",
    "\n",
    "reg = transformer_regression('/depot/jgmakin/data/auditory_cortex/josh_data/data',\"200213\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# channels = np.arange(0,reg.dataset.num_channels).tolist()\n",
    "num_layers = len(reg.layers)\n",
    "\n",
    "\n",
    "PCt =[]\n",
    "PCv =[]\n",
    "PCtt =[]\n",
    "w = 100\n",
    "ch=1\n",
    "sub = \"200213\"\n",
    "\n",
    "corr_values = {}\n",
    "for ch in range(4):\n",
    "    R2t =[]  \n",
    "    R2v =[] \n",
    "    R2tt =[]\n",
    "    for l in range(num_layers):\n",
    "        r2t, r2v,r2tt = reg.get_cc_norm(l,w,channel=ch, delay=0)\n",
    "        R2t.append(r2t.item())\n",
    "        R2v.append(r2v.item())\n",
    "        R2tt.append(r2tt.item())\n",
    "        # PCt.append(pct)\n",
    "        # PCv.append(pcv)\n",
    "        # PCtt.append(pctt)\n",
    "    corr_values[ch] =  {\"train\": R2t, \"val\": R2v, \"test\": R2tt}\n",
    "# fig, ax = plt.subplots(1,2, figsize=(14,6), sharey=True)\n",
    "\n",
    "with open(\"/scratch/gilbreth/akamsali/Research/Makin/Auditory_Cortex/\"+sub + \"_\" + str(w), 'a') as f:\n",
    "    json.dump(corr_values, f)\n",
    "\n",
    "    \n",
    "# plt.plot(R2t, label='Training')\n",
    "# plt.plot(R2v, label='Val')\n",
    "# plt.plot(R2tt, label='Test')\n",
    "# plt.legend()\n",
    "\n",
    "\n",
    "# ax[0].plot(R2t, label='Training')\n",
    "# ax[0].plot(R2v, label='Val')\n",
    "# ax[0].plot(R2tt, label='Test')\n",
    "# ax[0].legend()\n",
    "# ax[0].set_title(\"R2\")\n",
    "\n",
    "\n",
    "# ax[1].plot(PCt, label='Training')\n",
    "# ax[1].plot(PCv, label='Val')\n",
    "# ax[1].plot(PCtt, label='Test')\n",
    "# ax[1].legend()\n",
    "# ax[1].set_title(\"PC\")\n",
    "\n",
    "# plt.title(\"200213, bin_width=\"+ str(w))\n",
    "# # plt.savefig(\"/Users/akshita/Documents/Research/Makin/outputs/200213_\" + str(w))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from Dataset import Neural_Data\n",
    "\n",
    "\n",
    "neural_data = Neural_Data('/Users/akshita/Documents/Research/Makin/data',\"200206\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeated_sents = {}\n",
    "for i in range(499):\n",
    "    data = neural_data.retrieve_spikes_count_for_all_trials(i, w=100)\n",
    "    if data[1].shape[0] > 1:\n",
    "        repeated_sents[i] = data[1].shape\n",
    "print(repeated_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "# print(channel.shape)\n",
    "\n",
    "# sents = [12,13,32,43,56,163,212,218,287,308]\n",
    "sents = [12]\n",
    "# sent_vals = {}\n",
    "# wins = [1,5,10,20,30,40,50,75,100,200]\n",
    "wins = [10]\n",
    "for w in wins:\n",
    "    for sent in sents:\n",
    "        data = neural_data.retrieve_spikes_count_for_all_trials(sent, w=w)\n",
    "\n",
    "        for j in range(len(data)):\n",
    "            channel = data[j]\n",
    "            # print(channel.shape)\n",
    "            cnt = 0\n",
    "            val = []\n",
    "            p_r = []\n",
    "            for shift in [0]:#1, 5, 10]:\n",
    "                for i in range(11):\n",
    "                    cnt+=1.0\n",
    "                    ind = np.arange(0,11,1)\n",
    "                    b_ind = ind[i]\n",
    "                    a_ind = np.delete(ind, i)\n",
    "                    # print(a_ind, b_ind)\n",
    "                    a = np.mean(channel[a_ind, :], axis=0)\n",
    "                    b = channel[b_ind,:]\n",
    "                    b = np.roll(b, shift)\n",
    "                    \n",
    "                    res = stats.linregress(a, b)\n",
    "                    pearson_r = stats.pearsonr(a, b)\n",
    "                    val.append(res.rvalue**2)\n",
    "                    p_r.append(pearson_r[0]**2)\n",
    "\n",
    "                with open(\"/home/amy/Documents/Research/Makin/outputs/good_sessions/linreg_cs_win_\" + str(w)+ \"_\"+ str(sent) +\"_\" + str(shift)+\".csv\", 'a') as f:\n",
    "                    writer=csv.writer(f)\n",
    "                    row = val \n",
    "                    writer.writerow(row)\n",
    "                    f.close()\n",
    "    print(\"done: \", w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.arange(0,11,1)\n",
    "b_ind = ind[i]\n",
    "a_ind = np.delete(ind, i)\n",
    "print(a_ind, b_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "fig, ax = plt.subplots(4, len(wins),sharey=True,figsize=(5,6))\n",
    "\n",
    "for i, shift in enumerate([0, 1, 5, 10]):\n",
    "    for j, w in enumerate(wins):\n",
    "        for sent in sents:\n",
    "        # plt.figure()\n",
    "            d = pd.read_csv(\"/home/amy/Documents/Research/Makin/outputs/good_sessions/linreg_cs_win_\" + str(w)+ \"_\"+ str(sent) +\"_\" + str(shift)+ \".csv\", header=None)\n",
    "            d.columns = [\"channels\", \"reg_r2\", \"pearson_r2\"]\n",
    "\n",
    "            avg_r2 = d['reg_r2'].to_numpy()\n",
    "            pearson_r2 = d['pearson_r2'].to_numpy()\n",
    "            # print(avg_r2.shape, pearson_r2.shape)\n",
    "\n",
    "            # channels = d['channels'].to_numpy()\n",
    "            # plt.figure()\n",
    "            # ax[i, j].plot(channels, avg_r2, label=str(sent))\n",
    "            ax[i].plot(avg_r2-pearson_r2, label=str(sent))\n",
    "            ax[0].set_title(f\"Win={w}\")\n",
    "            ax[i].set_ylabel(f\"Shift={shift}\")\n",
    "            plt.ylim(-1.25, 0.8)\n",
    "        # plt.xlabel(\"channels\")\n",
    "    # plt.title(f\"Win = {w}\" , fontsize=16)plt.legend()\n",
    "        # plt.savefig(\"/home/amy/Documents/Research/Makin/outputs/correlation/linreg_cs_win_\" + str(w)+ \".png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j, w in enumerate(wins):\n",
    "    for i, shift in enumerate([0,1, 5, 10]):\n",
    "        plt.figure()\n",
    "        for sent in sents:\n",
    "            d = pd.read_csv(\"/home/amy/Documents/Research/Makin/outputs/correlation/linreg_cs_win_\" + str(w)+ \"_\"+ str(sent) +\"_\" + str(shift)+ \".csv\", header=None)\n",
    "            d.columns = [\"channels\", \"reg_r2\", \"pearson_r2\"]\n",
    "\n",
    "            avg_r2 = d['reg_r2'].to_list()\n",
    "            pearson_r2 = d['pearson_r2'].to_list()\n",
    "            channels = d['channels'].to_list()\n",
    "            # plt.figure()\n",
    "            plt.plot(channels, avg_r2, label=str(sent))\n",
    "            plt.title(f\"Win={w}, Shift={shift}\")\n",
    "            # ax[i,j].set_ylabel(f\"Shift={shift}\")\n",
    "            plt.ylim(-0.5, 0.4)\n",
    "        # plt.xlabel(\"channels\")\n",
    "    # plt.title(f\"Win = {w}\" , fontsize=16)plt.legend()\n",
    "        # plt.savefig(\"/home/amy/Documents/Research/Makin/outputs/correlation/linreg_cs_win_\" + str(w)+ \".png\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for sent in sents:\n",
    "    d2 = pd.read_csv(\"/home/amy/Documents/Research/Makin/outputs/correlation/linreg_loo_win_ms10_\"+ str(sent) +\".csv\", header=None)\n",
    "    d2.columns = [\"channels\", \"avg_r2\"]\n",
    "\n",
    "    avg_R2 = d2['avg_r2'].to_list()\n",
    "    channels = d2['channels'].to_list()\n",
    "    \n",
    "    plt.plot(channels, avg_R2 ,label=str(sent))\n",
    "    plt.ylim(-1.25, 0.8)\n",
    "plt.title(\"Correlation for sentences ms vs channels, w=10\", fontsize=16)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.zeros(100)\n",
    "b = np.zeros(100)\n",
    "ind_a = np.random.randint(0,100,10)\n",
    "ind_b = np.random.randint(0,100,10)\n",
    "\n",
    "a[ind_a] = 1    \n",
    "b[ind_b] = 1\n",
    "res = stats.linregress(a,b)\n",
    "\n",
    "res.rvalue**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in [1]:\n",
    "    for sent in [12]:\n",
    "        data = neural_data.retrieve_spikes_count_for_all_trials(sent, w=w)\n",
    "\n",
    "        for j in [20]:\n",
    "            channel = data[j]\n",
    "            # for shift in [1, 5, 10]:\n",
    "            for i in range(1):\n",
    "                plt.plot(channel[i], label=j)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from 62 channels loaded...!\n"
     ]
    }
   ],
   "source": [
    "from Dataset import Neural_Data\n",
    "\n",
    "dataset = Neural_Data(\"/depot/jgmakin/data/auditory_cortex\", \"200206\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30618,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = dataset.audio(0)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from scipy import linalg\n",
    "from transformers import Speech2TextForConditionalGeneration, Speech2TextProcessor\n",
    "from Dataset import Neural_Data\n",
    "from Feature_Extractors import Feature_Extractor_S2T\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from deepspeech_pytorch.model import DeepSpeech\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class transformer_regression():\n",
    "  def __init__(self, dir, subject):\n",
    "    self.dir = os.path.join(dir, subject)\n",
    "    self.dataset = Neural_Data(dir, subject)\n",
    "    # self.layers = [\"model.encoder.layers.0.fc2\", \"model.encoder.layers.1.fc2\", \"model.encoder.layers.2.fc2\",\"model.encoder.layers.3.fc2\",\n",
    "    #                \"model.encoder.layers.4.fc2\",\"model.encoder.layers.5.fc2\",\"model.encoder.layers.6.fc2\",\"model.encoder.layers.7.fc2\",\n",
    "    #                \"model.encoder.layers.8.fc2\",\"model.encoder.layers.9.fc2\"]\n",
    "    # self.model = Speech2TextForConditionalGeneration.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
    "    # self.processor = Speech2TextProcessor.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
    "    self.layers = [\"conv.seq_module.0\"]\n",
    "    self.model = DeepSpeech.load_from_checkpoint('/scratch/gilbreth/akamsali/Research/Makin/data/pretrained/librispeech_pretrained_v3.ckpt')\n",
    "    self.model_extractor = Feature_Extractor_S2T(self.model, self.layers)\n",
    "    print(\"Objects created, now loading Transformer layer features...!\")\n",
    "    self.features, self.demean_features = self.get_transformer_features()\n",
    "\n",
    "  def simply_spikes(self, sent_s=1, sent_e=499, ch=0, w = 40, delay=0):\n",
    "    spikes ={}\n",
    "    for x,i in enumerate(range(sent_s,sent_e)):\n",
    "      spikes[x] = torch.tensor(self.dataset.retrieve_spike_counts(sent=i, win=w, delay=delay ,early_spikes=False)[ch])\n",
    "    spikes = torch.cat([spikes[i] for i in range(sent_e - sent_s)], dim = 0).numpy()\n",
    "    return spikes\n",
    "\n",
    "  def demean_spikes(self, sent_s=1, sent_e=499, ch=0, w = 40):\n",
    "    spikes ={}\n",
    "    spk_mean = {}\n",
    "    for x,i in enumerate(range(sent_s,sent_e)):\n",
    "      spikes[x] = torch.tensor(self.dataset.retrieve_spike_counts(sent=i, win=40 ,early_spikes=False)[ch])\n",
    "      spk_mean[x] = torch.mean(spikes[x], dim = 0)\n",
    "      spikes[x] = spikes[x] - spk_mean[x]\n",
    "    spikes = torch.cat([spikes[i] for i in range(sent_e - sent_s)], dim = 0).numpy()\n",
    "    return spikes\n",
    "\n",
    "\n",
    "\n",
    "  def benchmark_r2_score(self, w = 40, sent = 12):\n",
    "    #These sentences have repeated trials...!\n",
    "    #sents = [12,13,32,43,56,163,212,218,287,308]\n",
    "    r2_scores = np.zeros(self.dataset.num_channels)\n",
    "    #trials = obj.dataset.get_trials(13)\n",
    "    spkk = self.dataset.retrieve_spike_counts_for_all_trials(sent=sent, w=w)\n",
    "  \n",
    "    for i in range(self.dataset.num_channels):\n",
    "      h1 = np.mean(spkk[i][0:6], axis=0)\n",
    "      h2 = np.mean(spkk[i][6:], axis=0)\n",
    "      r2_scores[i] = self.r2(h1,h2)\n",
    "    return r2_scores\n",
    "\n",
    "  def translate(self, aud, fs = 16000):\n",
    "    inputs_features = self.processor(aud,padding=True, sampling_rate=fs, return_tensors=\"pt\").input_features\n",
    "    generated_ids = self.model_extractor(inputs_features)\n",
    "\n",
    "  # def simply_stack(self, features):\n",
    "  #   features = torch.cat([features[i] for i in range(498)], dim=0)\n",
    "  #   return features\n",
    "\n",
    "  # def get_transformer_features(self):\n",
    "  #   sent_s = 1\n",
    "  #   sent_e = 499\n",
    "  #   features = [{} for _ in range(len(self.layers))]\n",
    "\n",
    "  #   feats = {}\n",
    "  #   for x, i in enumerate(range(sent_s, sent_e)):\n",
    "  #     self.translate(self.dataset.audio(i))\n",
    "  #     for j, l in enumerate(self.layers):\n",
    "  #       features[j][x] = self.model_extractor.features[l]\n",
    "    \n",
    "  #   for j, l in enumerate(self.layers):\n",
    "  #     feats[j] = torch.cat([features[j][i] for i in range(sent_e-sent_s)], dim=0).numpy()\n",
    "  #   return feats\n",
    "\n",
    "  def get_transformer_features(self, sent_s = 1, sent_e = 499):\n",
    "    \n",
    "    features = [{} for _ in range(len(self.layers))]\n",
    "    demean_features = [{} for _ in range(len(self.layers))]\n",
    "    f_mean = {}    \n",
    "    feats = {}\n",
    "    demean_feats = {}\n",
    "    for x, i in enumerate(range(sent_s, sent_e)):\n",
    "      self.translate(self.dataset.audio(i))\n",
    "      for j, l in enumerate(self.layers):\n",
    "        features[j][x] = self.model_extractor.features[l]\n",
    "        f_mean[x] = torch.mean(features[j][x], dim = 0)    \n",
    "        demean_features[j][x]= features[j][x] - f_mean[x]\n",
    "    for j, l in enumerate(self.layers):\n",
    "      feats[j] = torch.cat([features[j][i] for i in range(sent_e-sent_s)], dim=0).numpy()\n",
    "      demean_feats[j] = torch.cat([demean_features[j][i] for i in range(sent_e-sent_s)], dim=0).numpy()\n",
    "    return feats, demean_feats\n",
    "\n",
    "  # for i in range(498):\n",
    "  #   f_mean[i] = torch.mean(features[i], dim = 0)\n",
    "  #   features[i] = features[i] - f_mean[i]\n",
    "  # features = torch.cat([features[i] for i in range(498)], dim=0)\n",
    "\n",
    "  def down_sample_features(self, feats, k):\n",
    "    out = np.zeros((int(np.ceil(feats.shape[0]/k)),feats.shape[1]))\n",
    "    for i in range(out.shape[0]):\n",
    "      #Just add the remaining samples at the end...!\n",
    "      if (i == out.shape[0] -1):\n",
    "        out[i] = feats[k*i:, :].sum(axis=0)\n",
    "      else:  \n",
    "        out[i] = feats[k*i:k*(i+1), :].sum(axis=0)\n",
    "    return out\n",
    "\n",
    "  def down_sample_spikes(self, spks, k):\n",
    "    out = np.zeros(int(np.ceil(spks.shape[0]/k)))\n",
    "    for i in range(out.shape[0]):\n",
    "      #Just add the remaining samples at the end...!\n",
    "      if (i == out.shape[0] -1):\n",
    "        out[i] = spks[k*i:].sum(axis=0)\n",
    "      else:  \n",
    "        out[i] = spks[k*i:k*(i+1)].sum(axis=0)\n",
    "    return out\n",
    "\n",
    "  def compute_r2(self, layer, win):\n",
    "    k = int(win/40)    # 40 is the min, bin size for 'Speech2Text' transformer model \n",
    "    # print(f\"k = {k}\")\n",
    "    r2t = np.zeros(self.dataset.num_channels)\n",
    "    r2v = np.zeros(self.dataset.num_channels)\n",
    "    pct = np.zeros(self.dataset.num_channels)\n",
    "    pcv = np.zeros(self.dataset.num_channels)\n",
    "\n",
    "    #downsamples if k>1 \n",
    "    if k >1:\n",
    "      feats = self.down_sample_features(self.features[layer], k)\n",
    "    else:\n",
    "      feats = self.features[layer]\n",
    "\n",
    "    m = int(feats.shape[0] *0.75)\n",
    "    x_train = feats[0:m, :]\n",
    "    x_test = feats[m:, :]\n",
    "    \n",
    "    for i in range(self.dataset.num_channels):\n",
    "      y = self.simply_spikes(ch=i)\n",
    "      if k>1:\n",
    "        y = self.down_sample_spikes(y,k)\n",
    "      y_train = y[0:m]\n",
    "      y_test = y[m:]\n",
    "      B = self.regression_param(x_train, y_train)\n",
    " \n",
    "      r2t[i] = self.regression_score(x_train, y_train, B)\n",
    "      r2v[i] = self.regression_score(x_test, y_test, B)\n",
    "      pct[i] = (np.corrcoef(self.predict(x_train, B), y_train)[0,1])**2\n",
    "      pcv[i] = (np.corrcoef(self.predict(x_test, B), y_test)[0,1])**2\n",
    "    return r2t, r2v, pct, pcv\n",
    "\n",
    "\n",
    "\n",
    "  def compute_r2_channel(self, layer, win, channel, delay):\n",
    "    k = int(win/40)    # 40 is the min, bin size for 'Speech2Text' transformer model \n",
    "    # print(f\"k = {k}\")\n",
    "    r2t = np.zeros(1)\n",
    "    r2v = np.zeros(1)\n",
    "    r2tt = np.zeros(1)\n",
    "    pct = np.zeros(1)\n",
    "    pcv = np.zeros(1)\n",
    "    pctt = np.zeros(1)\n",
    "\n",
    "    #downsamples if k>1 \n",
    "    if k >1:\n",
    "      feats = self.down_sample_features(self.features[layer], k)\n",
    "    else:\n",
    "      feats = self.features[layer]\n",
    "\n",
    "    n1 = int(feats.shape[0] *0.75)\n",
    "    n2 = int(feats.shape[0] *0.90)\n",
    "    x_train = feats[0:n1, :]\n",
    "    x_val = feats[n1:n2, :]\n",
    "    x_test = feats[n2:, :]\n",
    "    \n",
    "    # for i in range(self.dataset.num_channels):\n",
    "    y = self.simply_spikes(ch=channel, delay=delay)\n",
    "    if k>1:\n",
    "      y = self.down_sample_spikes(y,k)\n",
    "    y_train = y[0:n1]\n",
    "    y_val = y[n1:n2]    \n",
    "    y_test = y[n2:]\n",
    "    B = self.regression_param(x_train, y_train)\n",
    "\n",
    "    r2t = self.regression_score(x_train, y_train, B)\n",
    "    r2v = self.regression_score(x_val, y_val, B)\n",
    "    r2tt = self.regression_score(x_test, y_test, B)\n",
    "    pct = np.corrcoef(self.predict(x_train, B), y_train)\n",
    "    pcv = np.corrcoef(self.predict(x_val, B), y_val)\n",
    "    pctt = np.corrcoef(self.predict(x_test, B), y_test)\n",
    "    pct = np.square(pct[0,1])\n",
    "    pcv = np.square(pcv[0,1])\n",
    "    pctt = np.square(pctt[0,1])\n",
    "    \n",
    "    return r2t, r2v,r2tt, pct, pcv,pctt\n",
    "  \n",
    "  def FE_r2_channel(self, layer, win, channel):\n",
    "    k = int(win/40)    # 40 is the min, bin size for 'Speech2Text' transformer model \n",
    "    print(f\"k = {k}\")\n",
    "    r2t = np.zeros(1)\n",
    "    r2v = np.zeros(1)\n",
    "    pct = np.zeros(1)\n",
    "    pcv = np.zeros(1)\n",
    "    #downsamples if k>1 \n",
    "    if k >1:\n",
    "      feats = self.down_sample_features(self.demean_features[layer], k)\n",
    "    else:\n",
    "      feats = self.demean_features[layer]\n",
    "\n",
    "    m = int(feats.shape[0] *0.75)\n",
    "    x_train = feats[0:m, :]\n",
    "    x_test = feats[m:, :]\n",
    "    \n",
    "    # for i in range(self.dataset.num_channels):\n",
    "    y = self.demean_spikes(ch=channel)\n",
    "    if k>1:\n",
    "      y = self.down_sample_spikes(y,k)\n",
    "    y_train = y[0:m]\n",
    "    y_test = y[m:]\n",
    "    B = self.regression_param(x_train, y_train)\n",
    "\n",
    "    r2t = self.regression_score(x_train, y_train, B)\n",
    "    r2v = self.regression_score(x_test, y_test, B)\n",
    "    pct = (np.corrcoef(self.predict(x_train, B), y_train)[0,1])**2\n",
    "    pcv = (np.corrcoef(self.predict(x_test, B), y_test)[0,1])**2\n",
    "    return r2t, r2v, pct, pcv\n",
    "\n",
    "\n",
    "  def r2(self, labels, predictions):\n",
    "    score = 0.0\n",
    "    mean = np.mean(labels)\n",
    "    denom = np.sum(np.square(labels - mean))\n",
    "    num = np.sum(np.square(labels - predictions))\n",
    "    score = 1 - num/denom\n",
    "    return score\n",
    "  def regression_score(self, X,y, B):\n",
    "    y_hat = self.predict(X,B)\n",
    "    return self.r2(y, y_hat)\n",
    "\n",
    "  def regression_param(self, X, y):\n",
    "    B = linalg.lstsq(X, y)[0]\n",
    "    return B\n",
    "  def predict(self, X, B):\n",
    "    return X@B\n",
    "\n",
    "  def get_pcs(self, layer, sents):\n",
    "      #layer = 0\n",
    "      #sents = [495, 496, 497]\n",
    "      feats = [{} for _ in sents]\n",
    "      feats_pcs = {}\n",
    "      for i, s in enumerate(sents):\n",
    "        feats[i], _ = self.get_transformer_features(s, s+1)\n",
    "      layer_features = self.features[layer]\n",
    "      m = layer_features.shape[0]\n",
    "      pc = PCA(n_components=2)\n",
    "      pc.fit(layer_features[:int(0.75*m),:])\n",
    "      for i, s in enumerate(sents):\n",
    "        feats_pcs[i] = pc.transform(feats[i][layer]) \n",
    "      return feats_pcs\n",
    "\n",
    "  def plot_pcs(self, l=0, sents=[495,496,497]):\n",
    "      c_maps = ['Greys', 'Purples', 'Blues', 'Oranges',\n",
    "                'Greens', 'Reds',\n",
    "                #'YlOrBr', 'YlOrRd', 'OrRd', 'PuRd', 'RdPu', 'BuPu',\n",
    "                #'GnBu', 'PuBu', 'YlGnBu', 'PuBuGn', 'BuGn', 'YlGn'\n",
    "                ] \n",
    "      leg_colors = ['tab:gray','tab:purple','tab:blue','tab:orange',\n",
    "                    'tab:green','tab:red']\n",
    "\n",
    "      pcs = self.get_pcs(layer=l, sents=sents)\n",
    "\n",
    "      for i in range(len(pcs.keys())):\n",
    "        shades = np.arange(0,pcs[i].shape[0])\n",
    "\n",
    "        plt.scatter(pcs[i][:,0], pcs[i][:,1], label=f\"sent: {sents[i]}\", cmap=c_maps[(i+2)%len(c_maps)], c=shades, vmin=-40, vmax=80)\n",
    "\n",
    "      leg = plt.legend(loc='best')\n",
    "      for i in range(len(pcs.keys())):\n",
    "        leg.legendHandles[i].set_color(leg_colors[(i+2)%len(leg_colors)])\n",
    "\n",
    "      plt.xlabel(f\"PC1\")\n",
    "      plt.ylabel(f\"PC2\")\n",
    "      plt.title(f\"{self.layers[l]}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "39ee5902541f8d9473741304b8cb1bb6d4326f8af27c9be63d89c1642eb0d066"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('research': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
