{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Speech2TextForConditionalGeneration, Speech2TextProcessor, Speech2TextModel, Speech2TextTokenizer\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "\n",
    "# from auditory_cortex.regression import transformer_regression\n",
    "from auditory_cortex.dataset import Neural_Data\n",
    "from auditory_ctx.models.speech_recognition import SpeechRecognitionModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = '/Users/akshita/Documents/Research/Makin/data'\n",
    "# subject = '200206'\n",
    "# reg = transformer_regression(data_path, subject)\n",
    "\n",
    "# def get_beta_eta(layer=0):\n",
    "#     train_list = np.arange(1,451).tolist()\n",
    "#     z_vals_train, n_vals_train = reg.get_layer_values_and_spikes(layer=layer, win=80, sent_list=train_list)\n",
    "#     # print(z_vals_train.shape, n_vals_train.shape)\n",
    "    \n",
    "#     beta = np.linalg.solve(z_vals_train.T.dot(z_vals_train), (z_vals_train.T).dot(n_vals_train))\n",
    "#     return beta, n_vals_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '/Users/akshita/Documents/Research/Makin/data'\n",
    "subject = '200206'\n",
    "neural_data = Neural_Data(dir, subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Speech2TextForConditionalGeneration.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
    "# processor = Speech2TextProcessor.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
    "\n",
    "# model = Speech2TextModel.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
    "# processor = Speech2TextTokenizer\n",
    "\n",
    "cnn_layers= 3\n",
    "rnn_layers= 5\n",
    "rnn_dim= 512\n",
    "n_class= 29\n",
    "n_features= 128\n",
    "stride= 2\n",
    "dropout= 0.1\n",
    "batch_size = 64\n",
    "char_path = '/Users/akshita/Documents/Research/Makin/auditory-ctx/conf/char.space.27'\n",
    "\n",
    "model = SpeechRecognitionModel(cnn_layers, rnn_layers, rnn_dim, n_class, n_features, stride, dropout)\n",
    "\n",
    "\n",
    "checkpoint = torch.load('/Users/akshita/Documents/Research/Makin/outputs/Uni_GRU_model/epoch_600.pt', map_location=torch.device('cpu')) \n",
    "\n",
    "# checkpoint = torch.load(\"/scratch/gilbreth/akamsali/Research/Makin/outputs/long/models/epoch_245.pt\")\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "aud = neural_data.audio(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akshita/opt/anaconda3/envs/research/lib/python3.9/site-packages/torchaudio/functional/functional.py:594: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "spect = torchaudio.transforms.MelSpectrogram()\n",
    "aud_tensor = torchaudio.load('/Users/akshita/Documents/Research/Makin/Auditory_Cortex/notebooks/test.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectogram = spect(aud_tensor[0]).unsqueeze(1).transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hook():\n",
    "    def __init__(self, model, layer_name, backward=False):\n",
    "\n",
    "        layer = dict([*model.named_modules()])[layer_name]\n",
    "        layer.__name__ = layer_name\n",
    "        # self.hook_inp = spect_inp.register_backward_hook()\n",
    "        # if backward == False:\n",
    "\n",
    "        self.hook_fwd = layer.register_forward_hook(self.hook_fn_f)\n",
    "        self.hook_bwd = layer.register_backward_hook(self.hook_fn_b)\n",
    "        # else:\n",
    "\n",
    "    def hook_fn_f(self, layer, input, output):\n",
    "        # print(\"i\", input)\n",
    "        # print(\"0\", output)\n",
    "\n",
    "        self.input_f = input\n",
    "        self.output_f = output#.requires_grad_(True)\n",
    "\n",
    "    def hook_fn_b(self, layer, input, output):\n",
    "        # print(\"i\", input)\n",
    "        # print(\"0\", output)\n",
    "\n",
    "        self.input_b = input\n",
    "        self.output_b = output\n",
    "    \n",
    "    \n",
    "    def close(self):\n",
    "        self.hook.remove()\n",
    "\n",
    "def get_gradients(net_in, net, layer):     \n",
    "    # aud_processed = processor(net_in, padding=True, sampling_rate=16000, return_tensors=\"pt\")['input_features']\n",
    "    # print(aud_processed)\n",
    "    net_in.requires_grad = True\n",
    "    # op = torch.optim.SGD([aud_processed], lr=0.1, momentum=0.9)\n",
    "    net.zero_grad()\n",
    "    hook = Hook(net, layer, backward=True)\n",
    "    net_out = net(net_in)\n",
    "    # print(hook.output_f)\n",
    "    loss = hook.output_f[0].mean()\n",
    "    loss.backward()\n",
    "\n",
    "    # print(hook.input_b, hook.output_b)\n",
    "    # op.step()\n",
    "    # print(net_in.grad)\n",
    "    # print(net_in.grad, net_in.grad_fn)\n",
    "\n",
    "    return net_in.grad.data\n",
    "\n",
    "def dream(audio, net, layer, iterations, lr):\n",
    "\n",
    "    for i in range(iterations):\n",
    "        gradients = get_gradients(audio, net, layer)\n",
    "        audio.data = audio.data + lr * gradients.data\n",
    "\n",
    "    audio_out = audio.detach().cpu()\n",
    "    \n",
    "    return audio_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:79] Could not initialize NNPACK! Reason: Unsupported hardware.\n",
      "/Users/akshita/opt/anaconda3/envs/research/lib/python3.9/site-packages/torch/nn/modules/module.py:1025: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[8.1675e-06, 2.1972e-05, 8.1675e-06,  ..., 2.1972e-05,\n",
       "           8.1675e-06, 6.6713e-06],\n",
       "          [5.4910e-05, 7.2356e-05, 6.0224e-05,  ..., 7.2559e-05,\n",
       "           1.3822e-04, 1.0164e-04],\n",
       "          [5.9077e-05, 8.5118e-05, 8.7691e-05,  ..., 8.6212e-05,\n",
       "           5.0765e-04, 4.5477e-04],\n",
       "          ...,\n",
       "          [5.0684e-05, 7.0149e-05, 8.0419e-05,  ..., 7.4073e-05,\n",
       "           5.9124e-05, 2.3174e-05],\n",
       "          [3.9668e-05, 3.5637e-05, 1.7201e-05,  ..., 3.5897e-05,\n",
       "           4.4854e-05, 3.0475e-05],\n",
       "          [1.0664e-04, 5.3002e-05, 3.7897e-05,  ..., 4.6879e-05,\n",
       "           4.0434e-05, 4.8521e-05]]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "dream(spectogram, model, \"cnn\", iterations=1, lr=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tf = Speech2TextForConditionalGeneration.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
    "# processor = Speech2TextProcessor.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Speech2TextForConditionalGeneration(\n",
       "  (model): Speech2TextModel(\n",
       "    (encoder): Speech2TextEncoder(\n",
       "      (conv): Conv1dSubsampler(\n",
       "        (conv_layers): ModuleList(\n",
       "          (0): Conv1d(80, 1024, kernel_size=(5,), stride=(2,), padding=(2,))\n",
       "          (1): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))\n",
       "        )\n",
       "      )\n",
       "      (embed_positions): Speech2TextSinusoidalPositionalEmbedding()\n",
       "      (layers): ModuleList(\n",
       "        (0): Speech2TextEncoderLayer(\n",
       "          (self_attn): Speech2TextAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): Speech2TextEncoderLayer(\n",
       "          (self_attn): Speech2TextAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): Speech2TextEncoderLayer(\n",
       "          (self_attn): Speech2TextAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): Speech2TextEncoderLayer(\n",
       "          (self_attn): Speech2TextAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): Speech2TextEncoderLayer(\n",
       "          (self_attn): Speech2TextAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): Speech2TextEncoderLayer(\n",
       "          (self_attn): Speech2TextAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): Speech2TextEncoderLayer(\n",
       "          (self_attn): Speech2TextAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): Speech2TextEncoderLayer(\n",
       "          (self_attn): Speech2TextAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): Speech2TextEncoderLayer(\n",
       "          (self_attn): Speech2TextAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): Speech2TextEncoderLayer(\n",
       "          (self_attn): Speech2TextAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): Speech2TextEncoderLayer(\n",
       "          (self_attn): Speech2TextAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): Speech2TextEncoderLayer(\n",
       "          (self_attn): Speech2TextAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): Speech2TextDecoder(\n",
       "      (embed_tokens): Embedding(10000, 256, padding_idx=1)\n",
       "      (embed_positions): Speech2TextSinusoidalPositionalEmbedding()\n",
       "      (layers): ModuleList(\n",
       "        (0): Speech2TextDecoderLayer(\n",
       "          (self_attn): Speech2TextAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): Speech2TextAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): Speech2TextDecoderLayer(\n",
       "          (self_attn): Speech2TextAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): Speech2TextAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): Speech2TextDecoderLayer(\n",
       "          (self_attn): Speech2TextAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): Speech2TextAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): Speech2TextDecoderLayer(\n",
       "          (self_attn): Speech2TextAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): Speech2TextAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): Speech2TextDecoderLayer(\n",
       "          (self_attn): Speech2TextAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): Speech2TextAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): Speech2TextDecoderLayer(\n",
       "          (self_attn): Speech2TextAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): Speech2TextAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=256, out_features=10000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dream(aud, model, \"encoder.conv.conv_layers.1\", iterations=1, lr=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aud_processed.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auditory_cortex.dataset import Neural_Data\n",
    "\n",
    "# dir = '/depot/jgmakin/data/auditory_cortex/josh_data/data'\n",
    "dir = '/Users/akshita/Documents/Research/Makin/data'\n",
    "subject = '200206'\n",
    "neural_data = Neural_Data(dir, subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import torch\n",
    "import os\n",
    "wav_path = '/Users/akshita/Documents/Research/Makin/data/wav_files'\n",
    "wav_files = list(map(lambda x: wav_path +'/sentence_' + str(x+1) +'.wav', range(500)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetFeatures:\n",
    "    def __init__(self, dir, subject, delay=0, model='transformer', VERBOSE=False):\n",
    "        self.model_name = model\n",
    "        if self.model_name == 'transformer':\n",
    "            self.model = TransformerModel()\n",
    "            self.neural_data = Neural_Data(dir, subject)\n",
    "\n",
    "            self.get_transformer_features()\n",
    "            if VERBOSE:\n",
    "                print(\"Got transformer features\")\n",
    "\n",
    "            self.get_spikes()\n",
    "            if VERBOSE:\n",
    "                print(\"Got spikes\")\n",
    "\n",
    "    def translate(self, aud, fs = 16000):\n",
    "        self.inputs_features = self.model.processor(aud,padding=True, sampling_rate=fs, return_tensors=\"pt\").input_features\n",
    "        generated_ids = self.model.model_extractor(self.inputs_features)\n",
    "\n",
    "    def get_transformer_features(self):\n",
    "        features = [{} for _ in range(len(self.model.layers))]\n",
    "        for i in range(0,500):\n",
    "            self.translate(self.neural_data.audio(i))\n",
    "\n",
    "            for j, l in enumerate(self.model.layers):\n",
    "                features[j][i] = self.model.model_extractor.features[l]\n",
    "\n",
    "        self.all_hidden_features = features\n",
    "        \n",
    "\n",
    "    def get_spikes(self, delay=0):\n",
    "        spikes = [{}, {}]\n",
    "        for i in range(1,500):\n",
    "            spikes[0][i] = self.neural_data.retrieve_spike_counts(sent=i, win=20, delay=delay, \n",
    "                                                                early_spikes=False,\n",
    "                                                                model=self.model_name, \n",
    "                                                                offset=-0.25)\n",
    "            spikes[1][i] = self.neural_data.retrieve_spike_counts(sent=i, win=40, delay=delay, \n",
    "                                                                early_spikes=False,\n",
    "                                                                model=self.model_name,\n",
    "                                                                offset=0.39)\n",
    "\n",
    "        self.all_spikes_dict = spikes"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "11ae26810785ba78c37a15c9eb002d2311c93b4a4adb47993511490e08153fac"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('research_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
