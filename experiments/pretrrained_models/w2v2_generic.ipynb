{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmedb/projects/Wav2Letter/deepspeech.pytorch/deepspeech_pytorch/loader/data_loader.py:17: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"sox_io\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, Wav2Vec2FeatureExtractor\n",
    "from auditory_cortex.dataloader import DataLoader\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wav2vec...!\n",
      "Calculating receptive fields for all layers...\n",
      "Layer 0, RF:    20 samples, 0.42 ms, sampling_rate: 4800Hz, sampling_time: 0.208ms\n",
      "Layer 1, RF:    60 samples, 1.25 ms, sampling_rate: 1600Hz, sampling_time: 0.625ms\n",
      "Layer 2, RF:   120 samples, 2.50 ms, sampling_rate: 800Hz, sampling_time: 1.250ms\n",
      "Layer 3, RF:   240 samples, 5.00 ms, sampling_rate: 400Hz, sampling_time: 2.500ms\n",
      "Layer 4, RF:   480 samples, 10.00 ms, sampling_rate: 200Hz, sampling_time: 5.000ms\n",
      "Layer 5, RF:   720 samples, 15.00 ms, sampling_rate: 100Hz, sampling_time: 10.000ms\n",
      "Layer 6, RF:  1200 samples, 25.00 ms, sampling_rate: 50Hz, sampling_time: 20.000ms\n"
     ]
    }
   ],
   "source": [
    "from auditory_cortex.utils import get_receptive_fields\n",
    "print(\"Wav2vec...!\")\n",
    "kernels = [20, 5, 3, 3, 3, 2, 2]\n",
    "strides = [10, 3, 2, 2, 2, 2, 2]\n",
    "# the last entries are kernel size and strides of the 'convolution position encoding'\n",
    "\n",
    "get_receptive_fields(kernels, strides, fs=48000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default normalizer file...\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader()\n",
    "aud = dataloader.metadata.stim_audio(sent=12)\n",
    "fs = dataloader.metadata.get_sampling_rate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model on device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'w2v2_generic'\n",
    "dnn_obj = dataloader.get_DNN_obj(model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_input = aud.astype(np.float64)\n",
    "input_values = dnn_obj.extractor.processor(\n",
    "\taudio_input, sampling_rate=48000, return_tensors=\"pt\", padding=\"longest\"\n",
    "\t).input_values  # Batch size 1\n",
    "dnn_obj.extractor.model.eval()\n",
    "# with torch.no_grad():\n",
    "input_values = input_values.to(dnn_obj.extractor.device)\n",
    "out = dnn_obj.extractor.model(input_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2BaseModelOutput(last_hidden_state=tensor([[[-0.7463,  0.3522, -0.1401,  ..., -0.6947,  1.1080, -1.1386],\n",
       "         [-0.6872,  0.3424, -0.1933,  ..., -0.6574,  1.1438, -1.2890],\n",
       "         [-0.9524,  0.7153,  0.1298,  ..., -0.4903,  0.9808, -1.2617],\n",
       "         ...,\n",
       "         [-1.1682,  0.6434,  0.1576,  ..., -0.7273,  1.1989, -1.2184],\n",
       "         [-0.3292,  0.7253, -0.3920,  ..., -0.5218,  1.1232, -1.6271],\n",
       "         [-0.7395,  0.3259, -0.0253,  ..., -0.7346,  1.1811, -1.1488]]],\n",
       "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), extract_features=tensor([[[-0.7850, -0.8867, -0.9281,  ..., -0.4476, -0.9006, -0.5120],\n",
       "         [-0.8131, -0.7833, -0.8405,  ..., -0.1075, -0.8504, -0.4853],\n",
       "         [-0.1379,  2.6510,  2.7070,  ...,  3.1893,  0.9478, -0.6239],\n",
       "         ...,\n",
       "         [-0.4617,  2.4437,  3.5720,  ...,  4.8108,  1.3308, -0.7093],\n",
       "         [-0.7013, -0.5920, -0.6433,  ...,  1.4962,  1.4987,  0.9948],\n",
       "         [-0.7077, -0.8635, -0.9188,  ..., -0.4738, -0.8700, -0.4984]]],\n",
       "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"bilalhsp/wav2vec2-48KHz-audioset-natual-sounds-v1\"\n",
    "cache_dir='/scratch/gilbreth/ahmedb/cache/huggingface/models/'\n",
    "\n",
    "model = AutoModel.from_pretrained(\n",
    "\tmodel_id,\n",
    "\tcache_dir=cache_dir,\n",
    "\t)\n",
    "\n",
    "processor =\tWav2Vec2FeatureExtractor.from_pretrained(\n",
    "\tmodel_id,\n",
    "\tcache_dir=cache_dir,\n",
    "\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_names = dict([*model.named_modules()])\n",
    "# ['wav2vec2.feature_extractor.conv_layers.0.layer_norm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2FeatureEncoder(\n",
       "  (conv_layers): ModuleList(\n",
       "    (0): Wav2Vec2GroupNormConvLayer(\n",
       "      (conv): Conv1d(1, 512, kernel_size=(20,), stride=(10,), bias=False)\n",
       "      (activation): GELUActivation()\n",
       "      (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "    )\n",
       "    (1): Wav2Vec2NoLayerNormConvLayer(\n",
       "      (conv): Conv1d(512, 512, kernel_size=(5,), stride=(3,), bias=False)\n",
       "      (activation): GELUActivation()\n",
       "    )\n",
       "    (2-4): 3 x Wav2Vec2NoLayerNormConvLayer(\n",
       "      (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "      (activation): GELUActivation()\n",
       "    )\n",
       "    (5-6): 2 x Wav2Vec2NoLayerNormConvLayer(\n",
       "      (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "      (activation): GELUActivation()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_names['feature_extractor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2Encoder(\n",
       "  (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "    (conv): ParametrizedConv1d(\n",
       "      768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "      (parametrizations): ModuleDict(\n",
       "        (weight): ParametrizationList(\n",
       "          (0): _WeightNorm()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (padding): Wav2Vec2SamePadLayer()\n",
       "    (activation): GELUActivation()\n",
       "  )\n",
       "  (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0-11): 12 x Wav2Vec2EncoderLayer(\n",
       "      (attention): Wav2Vec2SdpaAttention(\n",
       "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (feed_forward): Wav2Vec2FeedForward(\n",
       "        (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (intermediate_act_fn): GELUActivation()\n",
       "        (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_names['encoder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
