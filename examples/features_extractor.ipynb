{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmedb/.conda/envs/cent7/2020.11-py38/wav2letter_pretrained/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from auditory_cortex.feature_extractors import FeatureExtractorW2L\n",
    "from auditory_cortex.dataset import Neural_Data\n",
    "from wav2letter.models import Wav2LetterRF\n",
    "import wav2letter\n",
    "import numpy as np\n",
    "import torch\n",
    "import yaml\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating 'W2L_modified' object with weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = os.getcwd()\n",
    "conf_file = 'config_rf.yaml'\n",
    "manifest_file = os.path.join(os.path.dirname(wav2letter.__file__),\"conf\",conf_file)\n",
    "with open(manifest_file, 'r') as f:\n",
    "    model_param = yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model with pretrained weights....!\n",
    "checkpoint_file = \"Wav2letter-epoch=024-val_loss=0.37.ckpt\"\n",
    "checkpoint = os.path.join(model_param[\"results_dir\"],checkpoint_file)\n",
    "mod = Wav2LetterRF.load_from_checkpoint(checkpoint, manifest=model_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating feature extractor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = FeatureExtractorW2L(mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Neural Dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/depot/jgmakin/data/auditory_cortex/josh_data/data'\n",
    "sub = '200206'\n",
    "dataset = Neural_Data(data_dir, sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method get_features in module auditory_cortex.feature_extractors:\n",
      "\n",
      "get_features(layer_index) method of auditory_cortex.feature_extractors.FeatureExtractorW2L instance\n",
      "    Use to extract features for specific layer.\n",
      "    \n",
      "    Args:\n",
      "    \n",
      "        layer_index (int): layer index in the range [0, Total_Layers)\n",
      "    \n",
      "    returns:\n",
      "        (dim, time) features extracted for layer at 'layer_index' location\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(extractor.get_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method translate in module auditory_cortex.feature_extractors:\n",
      "\n",
      "translate(aud) method of auditory_cortex.feature_extractors.FeatureExtractorW2L instance\n",
      "    Forward passes audio input through the model and \n",
      "    captures the features in the 'self.features' dict.\n",
      "    \n",
      "    Args:\n",
      "        aud (ndarray): single 'wav' input of shape (t,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(extractor.translate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=12\n",
    "aud = dataset.audio(sent=s)\n",
    "feats = extractor.translate(aud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([137, 250])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractor.get_features(3).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (My wav2letter_pretrained Kernel)",
   "language": "python",
   "name": "wav2letter_pretrained"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
